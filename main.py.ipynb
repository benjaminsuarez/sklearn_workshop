{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "#' each row is an observation, each column is a feature\n",
    "print(iris.feature_names)\n",
    "print(iris.target)\n",
    "print(iris.target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#' in scikit-learn features and response are separate objects\n",
    "#' lets store feature matrix in X \n",
    "#' lets store response vector in y\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' scikit-learn is organised into modules, to make it easy to find classes.\n",
    "import sklearn\n",
    "dir(sklearn)\n",
    "\n",
    "#' scikit-learn has a 4 steps to modeling\n",
    "#' 1: import the class you want to use.\n",
    "#' 2: instantiate the class. Here you can specify tuning parameters.\n",
    "#' 3: fit / train the model\n",
    "#' 4: predict response for a new observation.\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 1)\n",
    "knn.fit(X,y)\n",
    "print(knn.predict([[3,5,4,2]]))\n",
    "X_new = [[3, 5, 4, 2], [5, 4, 3, 2]]\n",
    "print(knn.predict(X_new))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn.fit(X,y)\n",
    "X_new = [[3, 5, 4, 2], [5, 4, 3, 2]]\n",
    "print(knn.predict(X_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' did the model work well?\n",
    "#' can't  tell with 'out of sample' observations\n",
    "\n",
    "#' lets examine our training accuracy, ie. the proportion of correct predictions (an evaluation metric for classification problems)\n",
    "#' lets also compare training accuracy using different values of K.\n",
    "\n",
    "from sklearn import metrics\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn.fit(X,y)\n",
    "y_pred = knn.predict(X)\n",
    "print(metrics.accuracy_score(y,y_pred))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 1)\n",
    "knn.fit(X,y)\n",
    "y_pred = knn.predict(X)\n",
    "print(metrics.accuracy_score(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' Our goal is to estimate the likely performance of our model on out-of-sample-data.\n",
    "#' maximizing training accuracy rewards overfitting the model to the date and the model may not generalize.\n",
    "#' ie. the model may learn the noise more than the signal.\n",
    "#' the recommedation is to train-test-split the data...\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=4)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' testing accuracy is a better estimate for out-of-sample-performance then training accuracy!\n",
    "#' lets repeat what we did previously using our train-test-split.\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' how can we find a better value for K?\n",
    "#' lets iterate through a list of K values, and plot test accuracys for each value of K.\n",
    "k_range = list(range(1, 26))\n",
    "scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    scores.append(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(k_range, scores)\n",
    "plt.xlabel('values of K ')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' once model is chosen, and optimal parameters set, and ready to make predictions using out-of-sample-data\n",
    "#'remember to retrain the model using all the data. otherwise will be throwing away valuable training data\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=11)\n",
    "knn.fit(X, y) # using all the dataschool\n",
    "print(knn.predict([[3, 5, 4, 2]])) # an out-of-sample observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' # downsides of train-test-split:\n",
    "#' high variance of out-of-sample accuracy!! accuracy changes a lot depending on what happened to be in the training set.\n",
    "#' you can use k-fold cross validation instead of train-test-split to overcome this.\n",
    "\n",
    "#' lets look at advertising data using pandas!!\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv\", index_col = 0)\n",
    "print(data.head())\n",
    "print(data.tail())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' lets use seaborn to visualise the relationships between features and response, pairplots is good for this.\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.pairplot(data, x_vars = [\"TV\", \"radio\", \"newspaper\"], y_vars = \"sales\", height = 7, aspect = 0.7, kind = \"reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' we want to predict sales based on advertising hours, this is a regression problem.\n",
    "feature_cols = [\"TV\", \"radio\", \"newspaper\"]\n",
    "X = data[feature_cols] # subset original dataframe\n",
    "print(X.head)\n",
    "print(X.shape)\n",
    "print(type(X))\n",
    "\n",
    "y = data[\"sales\"] # data.sales also works\n",
    "print(y.head)\n",
    "print(y.shape)\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "print(X_train.shape) \n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "#' default split is 75% for training and 25% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' Although we'll be using a new class, we follow the same framework for generating our model\n",
    "#' 1: import the class you want to use.\n",
    "#' 2: instantiate the class. Here you can specify tuning parameters.\n",
    "#' 3: fit / train the model\n",
    "#' 4: predict response for a new observation.\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#' instantiate\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train) # learn the coefficients\n",
    "print(linreg.intercept_) # print the intercept and coefficients\n",
    "print(linreg.coef_)\n",
    "print(list(zip(feature_cols, linreg.coef_))) # can pair the feature names with the coefficients\n",
    "\n",
    "#' making predictions with linear model\n",
    "y_pred = linreg.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' we need some evaluation metric to compare our predictions with the actual values. can't use accuracy like in the classification problem before!\n",
    "\n",
    "#' Mean Absolute Error (MAE)\n",
    "#' it is the mean of the absolute value of the errors\n",
    "#' error is the difference between the true and predicted values\n",
    "#' a short example below:\n",
    "true = [100, 50, 30, 20]\n",
    "pred = [90, 50, 50, 30]\n",
    "print((10 + 0 + 20 + 10)/4.) # calculate MAE by hand\n",
    "\n",
    "from sklearn import metrics # calculate MAE using scikit-learn\n",
    "print(metrics.mean_absolute_error(true, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#' MSE: mean squared error\n",
    "print((10**2 + 0**2 + 20**2 + 10**2)/4.) # calculate MSE by hand\n",
    "print(metrics.mean_squared_error(true, pred))\n",
    "#' MSE is a bit harder to interpret than MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' RMSE: root mean squared error\n",
    "import numpy as np\n",
    "print(np.sqrt((10**2 + 0**2 + 20**2 + 10**2)/4.)) # by hand\n",
    "print(np.sqrt(metrics.mean_squared_error(true, pred)))\n",
    "#' notice RMSE is a bit larger than MAE, squaring of errors increases the weight of larger errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' The bottom line\n",
    "#' MAE: easiest to understand, it's an average error\n",
    "#' MSE: more popular than MAE, it punishes larger errors\n",
    "#' RMSE: more popular than MSE, it is interpretable in the \"y\" units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' compute RMSE for sales prediction\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "#' value of 1.4 is p.gud as sales ranged from 5 to 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' linear reg has no tuning parameters for us to tune.\n",
    "# can use train_test_split to look at individual features\n",
    "# lets remove newspapers, as it showed week correlation on visualisation\n",
    "feature_cols = ['TV', 'radio']\n",
    "X = data[feature_cols]\n",
    "y = data.sales\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "linreg.fit(X_train, y_train)\n",
    "y_pred = linreg.predict(X_test)\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "# RMSE reduces (error is what we want to minimize)\n",
    "#' it is unlikely this feature is useful for predicting sales and should be removed from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' purpose of model evaluation is to choose the best model: goal is estimate the likely performance of a model on out-of-sample data\n",
    "#' remember: downside of tran-test-split; high variance estimate of the out-of-sample testing accuracy\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# read in the iris data\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)\n",
    "\n",
    "#' lets check classification accuracy of KNN with K=5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' there's a difference in test set accuracy!\n",
    "#' testing accuracy is a high variance estimate like we said\n",
    "#' we can reduce this variance using cross validation\n",
    "\n",
    "#' k-fold cross-validation:\n",
    "#' split data into K folds\n",
    "#' use 1 fold as testing and remainder as training\n",
    "#' calculate test accuracy\n",
    "#' repeat K times with different folds!\n",
    "#' use average test accuracy as the estimate of out-of-sample accuracy\n",
    "\n",
    "#' simulation of splitting a dataset of 25 observations into 5 folds using KFold!\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=False).split(range(25))\n",
    "\n",
    "#' print the contents of each training and testing set from simulation\n",
    "print('{} {:^61} {}'.format('Iteration', 'Training set observations', 'Testing set observations'))\n",
    "for iteration, data in enumerate(kf, start=1):\n",
    "    print('{:^9} {} {:^25}'.format(iteration, data[0], str(data[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disadvantages of k cross-validation, difficult to inspect results with confusion matrix or ROC curve\n",
    "# these are easy to examine with train test split\n",
    "# recommendations: K=10, for classification problems, stratified sampling  is recommended for creating the folds, each response class should be represented with equal proportions in each of the K folds, \"cross-val-score\" does this by default\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#' 10-fold cross-validation with K=5 for KNN (the n_neighbors parameter)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "print(scores)\n",
    "#' use average accuracy as an estimate of out-of-sample accuracy\n",
    "print(scores.mean())\n",
    "\n",
    "#' search for an optimal value of K for KNN\n",
    "k_range = list(range(1, 31))\n",
    "k_scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "    k_scores.append(scores.mean())\n",
    "print(k_scores)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validated Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# grid_search_cv can do all the above for us!\n",
    "# k=13 to k=20 seems to work well for KNN, advised to choose simplest model. in KNN higher values of K produce simpler models, as such K=20 would be best.\n",
    "\n",
    "# lets use cross validation to choose between models!\n",
    "# 10-fold cross-validation with the best KNN model (after model tuning)\n",
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "print(cross_val_score(knn, X, y, cv=10, scoring='accuracy').mean())\n",
    "# 10-fold cross-validation with logistic regression (no tuning required)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "print(cross_val_score(logreg, X, y, cv=10, scoring='accuracy').mean())\n",
    "# can you tell which model performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, lets use cross-validation for feature selection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "data = pd.read_csv(\"http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv\", index_col=0)\n",
    "feature_cols = ['TV', 'radio', 'newspaper']\n",
    "X = data[feature_cols]\n",
    "y = data.sales\n",
    "\n",
    "# 10-fold cross-validation with all three features\n",
    "lm = LinearRegression()\n",
    "scores = cross_val_score(lm, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "mse_scores = -scores # fix sign MSE scores as cross_val_score negates it\n",
    "rmse_scores = np.sqrt(mse_scores) # convert from MSE to RMSE\n",
    "print(rmse_scores)\n",
    "print(rmse_scores.mean()) # calculate the average RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-fold cross-validation with two features (excluding Newspaper)\n",
    "feature_cols = ['TV', 'radio']\n",
    "X = data[feature_cols]\n",
    "print(np.sqrt(-cross_val_score(lm, X, y, cv=10, scoring='neg_mean_squared_error')).mean())\n",
    "# results in lower score, which we are trying to minimize, yay!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' now, lets learn how to use grid search to select the optimal tuning parameters\n",
    "#' use grid_search_cv for finding optimal K for knn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#' define the parameter values that should be searched\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "k_range = list(range(1, 31))\n",
    "print(k_range)\n",
    "#' create a parameter grid: map the parameter names to the values that should be searched\n",
    "param_grid = dict(n_neighbors=k_range)\n",
    "print(param_grid)\n",
    "#' instantiate the grid, with our options\n",
    "grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy', return_train_score=False)\n",
    "\n",
    "#' fit the grid with data, 10 fold cross val is being run 30X times, therofore model is being fit and predictions made 300 times\n",
    "grid.fit(X, y)\n",
    "#' view the results as a pandas DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "print(pd.DataFrame(grid.cv_results_)[['mean_test_score', 'std_test_score', 'params']])\n",
    "print(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' examine the first result\n",
    "print(grid.cv_results_['params'][0])\n",
    "print(grid.cv_results_['mean_test_score'][0])\n",
    "#' print the array of mean scores only\n",
    "grid_mean_scores = grid.cv_results_['mean_test_score']\n",
    "print(grid_mean_scores)\n",
    "#' plot the results\n",
    "plt.plot(k_range, grid_mean_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validated Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' examine the best model\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' can use param_grid to map multiple parameters at the same time!!\n",
    "#' define the parameter values that should be searched\n",
    "k_range = list(range(1, 31))\n",
    "weight_options = ['uniform', 'distance']\n",
    "#' create a parameter grid: map the parameter names to the values that should be searched\n",
    "param_grid = dict(n_neighbors=k_range, weights=weight_options)\n",
    "print(param_grid)\n",
    "#' instantiate and fit the grid, this is known as as exhaustive search as all parameters checked\n",
    "grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy', return_train_score=False)\n",
    "grid.fit(X, y)\n",
    "#' view the results\n",
    "print(pd.DataFrame(grid.cv_results_)[['mean_test_score', 'std_test_score', 'params']])\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' we have successfully tuned our parameters!\n",
    "#' now lets retrain our model with the best parameters on all the data.\n",
    "#' GridSearchCV automatically refits best model using all of the data, accessable with .predict\n",
    "print(grid.predict([[3, 5, 4, 2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' It can get computationally intensive to perform an exhaustiv tune.\n",
    "#' RandomizedGridCV solves this\n",
    "#' it searches only a random subset of the provided parameters. can effectively decide how long you want it to run for\n",
    "#' with RandomizedGridCV you provide a parameter distribution rather than a grid. lets do that.\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_dist = dict(n_neighbors=k_range, weights=weight_options)\n",
    "#' n_iter controls the number of searches\n",
    "rand = RandomizedSearchCV(knn, param_dist, cv=10, scoring='accuracy', n_iter=10, random_state=5, return_train_score=False)\n",
    "rand.fit(X, y)\n",
    "print(pd.DataFrame(rand.cv_results_)[['mean_test_score', 'std_test_score', 'params']])\n",
    "print(rand.best_score_)\n",
    "print(rand.best_params_)\n",
    "#' run RandomizedSearchCV 20 times (with n_iter=10) and record the best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' most of the time it's able to find the best if not closest to the best\n",
    "\n",
    "best_scores = []\n",
    "for _ in range(20):\n",
    "    rand = RandomizedSearchCV(knn, param_dist, cv=10, scoring='accuracy', n_iter=10, return_train_score=False)\n",
    "    rand.fit(X, y)\n",
    "    best_scores.append(round(rand.best_score_, 3))\n",
    "print(best_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' lets explore a classification problem more in depth using a diabetes dataset.\n",
    "#' the original dataset was used to predict diabetes based on diagnostic measurements, using data collected from pima native americans.\n",
    "import pandas as pd\n",
    "path = 'https://raw.githubusercontent.com/benjaminsuarez/sklearn_workshop/master/diabetes.csv'\n",
    "pima = pd.read_csv(path)\n",
    "\n",
    "#' lets try to predict the diabetes status of a patient given their health measurements\n",
    "\n",
    "#' define X and y\n",
    "feature_cols = ['Pregnancies', 'Insulin', 'BMI', 'Age']\n",
    "X = pima[feature_cols]\n",
    "y = pima.Outcome\n",
    "#' split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "#' train a logistic regression model on the training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "#' make class predictions for the testing set\n",
    "y_pred_class = logreg.predict(X_test)\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' beware: whenever using classification accuracy as evaluation metric, need to compare it to its null accuracy, which is the accuracy that could be achieved by always predicting the most frequent class in the testing set.\n",
    "#' null accuracy says, if I model were to predict the most dominant all the time, how often would it be correct\n",
    "\n",
    "#' null accuracy calculation:\n",
    "#' examine the class distribution of the testing set (using a Pandas Series method)\n",
    "print(y_test.value_counts()) # this is known as the class distribution\n",
    "print(y_test.mean()) # calculate the percentage of ones\n",
    "print(1 - y_test.mean()) # calculate the percentage of zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(y_test.mean(), 1 - y_test.mean())) # calculate null accuracy (for binary classification problems coded as 0/1)\n",
    "#' calculate null accuracy (for multi-class classification problems)\n",
    "print(y_test.value_counts().head(1) / len(y_test) )\n",
    "#' this highlights one weaknes of test accurracy as a model evaluation metric. It doesn't tell us anything about the underlying distribution of the testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' another weakness, when true is a zero, model almost always predicts zero, but when true value is one, model rarely predicts a one. it makes certain types of errors, but not others.\n",
    "#' ie. it doesn't tell you what type of errors it makes\n",
    "print('True:', y_test.values[0:25])\n",
    "print('Pred:', y_pred_class[0:25])\n",
    "#' this issue is remedied through the use of a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' confusion matrix:\n",
    "#' NB! first argument: true values, second argument: predicted values\n",
    "print(metrics.confusion_matrix(y_test, y_pred_class))\n",
    "\n",
    "#' True Positives (TP): we correctly predicted  they do have diabetes\n",
    "#' True Negatives (TN): we correctly predicted  they don't have diabetes\n",
    "#' False Positives (FP): we incorrectly predicted  they do have diabetes (a \"Type I error\")\n",
    "#' False Negatives (FN): we incorrectly predicted  they don't have diabetes (a \"Type II error\")\n",
    "\n",
    "#' note, confusion matrix is not an evaluation metric so cannot be used to choose model.\n",
    "confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' popular mertrics computed from a confusion matrix below\n",
    "#' classification accuracy:\n",
    "#' note need one number as float for true devision,vs. integer division.\n",
    "#' accuracy_score method does the exact same thing\n",
    "print((TP + TN) / float(TP + TN + FP + FN))\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "#' classification error: aka misclassification rate\n",
    "print((FP + FN) / float(TP + TN + FP + FN))\n",
    "print(1 - metrics.accuracy_score(y_test, y_pred_class))\n",
    "#' sensitivity:when actual value is positive, how often is prediction correct, aka recall_score\n",
    "print(TP / float(TP + FN))\n",
    "print(metrics.recall_score(y_test, y_pred_class))\n",
    "#' specificity: when actual value is neg, how often is the prediction correct. want to maximize this\n",
    "print(TN / float(TN + FP))\n",
    "#' false positive rate:\n",
    "print(FP / float(TN + FP))\n",
    "#' precision: when a positive value is predicted, how often is the prediction correct?\n",
    "print(TP / float(TP + FP))\n",
    "print(metrics.precision_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' Confusion matrix gives you a more complete picture of how your classifier is performing\n",
    "#' Also allows you to compute various classification metrics, and these metrics can guide your model selection\n",
    "\n",
    "#' we can adjust classification thresholds\n",
    "print(logreg.predict(X_test)[0:10]) # first 10 predicted responses\n",
    "#' print the first 10 predicted probabilities of class membership, row=observations, cols=class 0,1\n",
    "#' can be used to rank observations by predicted prob of diabetes and prioritize patient outreach.\n",
    "print(logreg.predict_proba(X_test)[0:10, :])\n",
    "#' print the first 10 predicted probabilities for class 1\n",
    "print(logreg.predict_proba(X_test)[0:10, 1])\n",
    "#' store the predicted probabilities for class 1, using all data\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:, 1]\n",
    "import matplotlib.pyplot as plt\n",
    "#' histogram of predicted probabilities\n",
    "plt.hist(y_pred_prob, bins=8)\n",
    "plt.xlim(0, 1)\n",
    "plt.title('Histogram of predicted probabilities')\n",
    "plt.xlabel('Predicted probability of diabetes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' note prediction threshold is set to 0.5, can adjust sensitivity and specificity by adjusting this\n",
    "#' predict diabetes if the predicted probability is greater than 0.3\n",
    "from sklearn.preprocessing import binarize\n",
    "y_pred_class = binarize([y_pred_prob], 0.3)[0]\n",
    "print(y_pred_prob[0:10]) # first 10 predicted probabilities\n",
    "print(y_pred_class[0:10]) # first 10 predicted classes with the lower threshold\n",
    "print(confusion) # previous confusion matrix (default threshold of 0.5)\n",
    "print(metrics.confusion_matrix(y_test, y_pred_class)) # threshold of 0.3\n",
    "print(46 / float(46 + 16))# sensitivity has increased (used to be 0.24)\n",
    "print(80 / float(80 + 50)) # specificity has decreased (used to be 0.91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' note, threshold adjustment is the last step you take in model building process\n",
    "#' majority of time should be spent selecting better models and choosing the best model\n",
    "#' can use ROC curve to see how sensitivity and specificity are affected by various thresholds, without actually changing the threshold.\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title('ROC curve for diabetes classifier')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "#' graph tells us, if you choose sensitivity of 0.9, must accept specificity of 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' lets define a function that accepts a threshold and prints sensitivity and specificity\n",
    "def evaluate_threshold(threshold):\n",
    "    print('Sensitivity:', tpr[thresholds > threshold][-1])\n",
    "    print('Specificity:', 1 - fpr[thresholds > threshold][-1])\n",
    "evaluate_threshold(0.5)\n",
    "evaluate_threshold(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#' AUC is % of the ROC plot under the curve\n",
    "#' AUC is useful as a single number summary of classifier performance.\n",
    "#' higher AUC is indicative of a better classifier, can be used as alternative to classification accuracy\n",
    "print(metrics.roc_auc_score(y_test, y_pred_prob))\n",
    "#' If you randomly chose one positive and one negative observation, AUC represents the likelihood that your classifier will assign a higher predicted probability to the positive observation.\n",
    "#' AUC is useful even when there is high class imbalance (unlike classification accuracy).\n",
    "#' in this scenario, AUC would be a useful evaluation metric, whereas classification accuracy, would not.\n",
    "#' calculate cross-validated AUC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print(cross_val_score(logreg, X, y, cv=10, scoring='roc_auc').mean())\n",
    "#' main advantage of ROC and AUC, don't require you to choose classification threshold unlike confusion matrix. however, they are less interpretable than confusion matrix, for multiclass problems\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
